/*
 * Copyright (c) 2024, Niklas Hauser
 *
 * This file is part of the modm project.
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/.
 */
// ----------------------------------------------------------------------------

#pragma once

#include <modm/platform/core/atomic_lock_impl.hpp>

/* We are implementing the libary interface described here:
 * See https://gcc.gnu.org/wiki/Atomic/GCCMM/LIbrary
 *
 * This header file must be included with <atomic>!
 */

[[gnu::always_inline]] inline void
__modm_atomic_pre_barrier([[maybe_unused]] int memorder)
{
%% if is_multicore
	// On Cortex-M, this will emit a DMB instruction and a compiler fence
	switch (memorder) {
		case __ATOMIC_RELEASE:
		case __ATOMIC_ACQ_REL:
		case __ATOMIC_SEQ_CST:
			__atomic_thread_fence(__ATOMIC_SEQ_CST);
	}
%% else
	// A compiler fence is enough
	__atomic_signal_fence(__ATOMIC_SEQ_CST);
%% endif
}

[[gnu::always_inline]] inline void
__modm_atomic_post_barrier([[maybe_unused]] int memorder)
{
%% if is_multicore
	// On Cortex-M, this will emit a DMB instruction and a compiler fence
	switch (memorder) {
		case __ATOMIC_CONSUME:
		case __ATOMIC_ACQUIRE:
		case __ATOMIC_ACQ_REL:
		case __ATOMIC_SEQ_CST:
			__atomic_thread_fence(__ATOMIC_SEQ_CST);
	}
%% else
	// A compiler fence is enough
	__atomic_signal_fence(__ATOMIC_SEQ_CST);
%% endif

}

// ============================= generic integers =============================
template<typename T>
[[gnu::always_inline]] inline T
__modm_atomic_load_t(const volatile void *ptr, int memorder)
{
	T value{};
	__modm_atomic_pre_barrier(memorder);
	{
		modm::atomic::Lock _;
		value = *reinterpret_cast<const volatile T*>(ptr);
	}
	__modm_atomic_post_barrier(memorder);
	return value;
}

%% macro atomic_load(len)
extern "C" [[gnu::always_inline]] inline {{len|u}}
__atomic_load_{{len//8}}(const volatile void *ptr, int memorder)
{
	return __modm_atomic_load_t<{{len|u}}>(ptr, memorder);
}
%% endmacro

template<typename T>
[[gnu::always_inline]] inline void
__modm_atomic_store_t(volatile void *ptr, T value, int memorder)
{
	__modm_atomic_pre_barrier(memorder);
	{
		modm::atomic::Lock _;
		*reinterpret_cast<volatile T*>(ptr) = value;
	}
	__modm_atomic_post_barrier(memorder);
}

%% macro atomic_store(len)
extern "C" [[gnu::always_inline]] inline void
__atomic_store_{{len//8}}(volatile void *ptr, {{len|u}} value, int memorder)
{
	__modm_atomic_store_t<{{len|u}}>(ptr, value, memorder);
}
%% endmacro

template<typename T>
[[gnu::always_inline]] inline T
__modm_atomic_exchange_t(volatile void *ptr, T desired, int memorder)
{
	T previous{};
	__modm_atomic_pre_barrier(memorder);
	{
		modm::atomic::Lock _;
		previous = *reinterpret_cast<volatile T*>(ptr);
		*reinterpret_cast<volatile T*>(ptr) = desired;
	}
	__modm_atomic_post_barrier(memorder);
	return previous;
}

%% macro atomic_exchange(len)
extern "C" [[gnu::always_inline]] inline {{len|u}}
__atomic_exchange_{{len//8}}(volatile void *ptr, {{len|u}} desired, int memorder)
{
	return __modm_atomic_exchange_t<{{len|u}}>(ptr, desired, memorder);
}
%% endmacro

template<typename T>
[[gnu::always_inline]] inline bool
__modm_atomic_compare_exchange_t(volatile void *ptr, void *expected, T desired, bool weak,
						  int success_memorder, int failure_memorder)
{
	bool retval{false};
	__modm_atomic_pre_barrier(weak ? success_memorder : __ATOMIC_SEQ_CST);
	{
		modm::atomic::Lock _;
		const T current = *reinterpret_cast<volatile T*>(ptr);
		if (current == *reinterpret_cast<T*>(expected)) [[likely]]
		{
			*reinterpret_cast<volatile T*>(ptr) = desired;
			retval = true;
		}
		else *reinterpret_cast<T*>(expected) = current;
	}
	__modm_atomic_post_barrier(weak ? (retval ? success_memorder : failure_memorder) : __ATOMIC_SEQ_CST);
	return retval;
}

%% macro atomic_compare_exchange(len)
extern "C" [[gnu::always_inline]] inline bool
__atomic_compare_exchange_{{len//8}}(volatile void *ptr, void *expected, {{len|u}} desired,
							bool weak, int success_memorder, int failure_memorder)
{
	return __modm_atomic_compare_exchange_t<{{len|u}}>(
			ptr, expected, desired, weak, success_memorder, failure_memorder);
}
%% endmacro

// ================================ lock free =================================
extern "C" [[gnu::always_inline]] inline bool
__atomic_is_lock_free (unsigned int object_size, const volatile void *ptr)
{
	// only lock free if size â‰¤ bus width and then also properly aligned
	if (object_size <= {{bus_width//8}}) [[likely]]
		return ((uintptr_t)ptr & (object_size - 1)) == 0;
	return false;
}


%% macro atomic_fetch(len)
	%% for name, op in [("and", "&"), ("or", "|"), ("xor", "^"), ("nand", "&")]
		%% set prefix = "~" if name == "nand" else ""
extern "C" [[gnu::always_inline]] inline {{len|u}}
__atomic_fetch_{{name}}_{{len//8}}(volatile void *ptr, {{len|u}} value, int memorder)
{
	{{len|u}} previous{};
	__modm_atomic_pre_barrier(memorder);
	{
		modm::atomic::Lock _;
		previous = *reinterpret_cast<volatile {{len|u}}*>(ptr);
		*reinterpret_cast<volatile {{len|u}}*>(ptr) = {{prefix}}(previous {{op}} value);
	}
	__modm_atomic_post_barrier(memorder);
	return previous;
}
	%% endfor
%% endmacro

%% for length in bit_lengths
// ========================= atomics for {{length}} bit integers =========================
%% if length > bus_width
{{ atomic_load(length) }}

{{ atomic_store(length) }}
%% endif

{{ atomic_exchange(length) }}

{{ atomic_compare_exchange(length) }}

{{ atomic_fetch(length) }}
%% endfor
